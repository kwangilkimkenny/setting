{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 2 3 2 3 1 2 1 1 2 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 5 1 1 1 1 1 1 1 4 1 1 2]]\n",
      "{'as': 2, 'the': 42, 'sun': 41, 'set': 35, 'in': 21, 'evening': 11, 'sky': 38, 'malcolm': 25, 'slowly': 39, 'turned': 47, 'and': 1, 'walked': 49, 'toward': 46, 'his': 19, 'home': 20, 'all': 0, 'was': 50, 'silent': 37, 'still': 40, 'through': 43, 'window': 52, 'he': 17, 'could': 9, 'see': 33, 'older': 29, 'brother': 5, 'james': 23, 'watching': 51, 'football': 14, 'game': 16, 'on': 30, 'tv': 48, 'from': 15, 'first': 13, 'year': 53, 'of': 28, 'college': 8, 'city': 7, 'it': 22, 'lonely': 24, 'at': 3, 'times': 44, 'but': 6, 'felt': 12, 'rather': 31, 'nice': 26, 'to': 45, 'not': 27, 'be': 4, 'shadow': 36, 'during': 10, 'senior': 34, 'high': 18, 'school': 32}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['As the sun set in the evening sky, Malcolm slowly turned and walked toward his home. All was silent and still. Through the window, he could see his older brother James watching a football game on the TV. James was home from his first year of college in the city. It was lonely at times, but Malcolm felt it was rather nice to not be in James shadow during his senior year of high school.']\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "# 코퍼스로부터 각 단어의 빈도 수를 기록\n",
    "\n",
    "print(vector.vocabulary_)\n",
    "# 단어별 인덱스 값 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFD-IDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08391814 0.16783627 0.08391814 0.08391814 0.08391814 0.08391814\n",
      "  0.08391814 0.08391814 0.08391814 0.08391814 0.08391814 0.08391814\n",
      "  0.08391814 0.08391814 0.08391814 0.08391814 0.08391814 0.08391814\n",
      "  0.08391814 0.33567254 0.16783627 0.25175441 0.16783627 0.25175441\n",
      "  0.08391814 0.16783627 0.08391814 0.08391814 0.16783627 0.08391814\n",
      "  0.08391814 0.08391814 0.08391814 0.08391814 0.08391814 0.08391814\n",
      "  0.08391814 0.08391814 0.08391814 0.08391814 0.08391814 0.08391814\n",
      "  0.41959068 0.08391814 0.08391814 0.08391814 0.08391814 0.08391814\n",
      "  0.08391814 0.08391814 0.33567254 0.08391814 0.08391814 0.16783627]]\n",
      "{'as': 2, 'the': 42, 'sun': 41, 'set': 35, 'in': 21, 'evening': 11, 'sky': 38, 'malcolm': 25, 'slowly': 39, 'turned': 47, 'and': 1, 'walked': 49, 'toward': 46, 'his': 19, 'home': 20, 'all': 0, 'was': 50, 'silent': 37, 'still': 40, 'through': 43, 'window': 52, 'he': 17, 'could': 9, 'see': 33, 'older': 29, 'brother': 5, 'james': 23, 'watching': 51, 'football': 14, 'game': 16, 'on': 30, 'tv': 48, 'from': 15, 'first': 13, 'year': 53, 'of': 28, 'college': 8, 'city': 7, 'it': 22, 'lonely': 24, 'at': 3, 'times': 44, 'but': 6, 'felt': 12, 'rather': 31, 'nice': 26, 'to': 45, 'not': 27, 'be': 4, 'shadow': 36, 'during': 10, 'senior': 34, 'high': 18, 'school': 32}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['As the sun set in the evening sky, Malcolm slowly turned and walked toward his home. All was silent and still. Through the window, he could see his older brother James watching a football game on the TV. James was home from his first year of college in the city. It was lonely at times, but Malcolm felt it was rather nice to not be in James shadow during his senior year of high school.']\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSA를 사용해서 문서의 수를 원하는 토픽의 수로 압축한 뒤에 각 토픽당 가장 중요한 단어 5개를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents= dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리한다. 알파벳을 제외한 모든 문자를 공백(space)로 처리하는 작업을 수행\n",
    "#짧은 단어는 유용한 정보를 담고 있지 않다고 가정하고, 길이가 짧은 단어도 제거\n",
    "#알파벳을 소문자로 바꿔서 단어의 개수 줄이기\n",
    "news_df = pd.DataFrame({'document': documents})\n",
    "# 알파벳을 제외하고 모두 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\",\" \")\n",
    "# 길이가 3이하인 단어는 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well sure about story seem biased what disagree with your statement that media ruin israels reputation that rediculous media most israeli media world having lived europe realize that incidences such described letter have occured media whole seem ignore them subsidizing israels existance europeans least same degree think that might reason they report more clearly atrocities what shame that austria daily reports inhuman acts commited israeli soldiers blessing received from government makes some holocaust guilt away after look jews treating other races when they power unfortunate'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어제거 및 토크나이즈하여 단어를 분리함\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x : x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'sure', 'story', 'seem', 'biased', 'disagree', 'statement', 'media', 'ruin', 'israels', 'reputation', 'rediculous', 'media', 'israeli', 'media', 'world', 'lived', 'europe', 'realize', 'incidences', 'described', 'letter', 'occured', 'media', 'whole', 'seem', 'ignore', 'subsidizing', 'israels', 'existance', 'europeans', 'least', 'degree', 'think', 'might', 'reason', 'report', 'clearly', 'atrocities', 'shame', 'austria', 'daily', 'reports', 'inhuman', 'acts', 'commited', 'israeli', 'soldiers', 'blessing', 'received', 'government', 'makes', 'holocaust', 'guilt', 'away', 'look', 'jews', 'treating', 'races', 'power', 'unfortunate']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF 행렬 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#역토큰화(Detokenization)\n",
    "#TfidfVectorizer는 기본적으로 토큰화가 되어있지 않은 텍스트 데이터를 입력으로 사용한다. \n",
    "#그렇기 때문에 TfidfVectorizer를 사용해서 TF-IDF 행렬을 만들기 위해서\n",
    "#다시 토큰화 작업을 역으로 취소하는 작업을 수행\n",
    "\n",
    "# 역토큰화 (토큰화 작업을 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well sure story seem biased disagree statement media ruin israels reputation rediculous media israeli media world lived europe realize incidences described letter occured media whole seem ignore subsidizing israels existance europeans least degree think might reason report clearly atrocities shame austria daily reports inhuman acts commited israeli soldiers blessing received government makes holocaust guilt away look jews treating races power unfortunate'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                            max_features=1000,\n",
    "                            max_df = 0.5,\n",
    "                            smooth_idf = True)\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape\n",
    "(11314, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=20,\n",
    "                        algorithm='randomized',\n",
    "                        n_iter=100,\n",
    "                        random_state=122)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('like', 0.2138), ('know', 0.20031), ('people', 0.19334), ('think', 0.17802), ('good', 0.15105)]\n",
      "Topic 2: [('thanks', 0.32918), ('windows', 0.29093), ('card', 0.18016), ('drive', 0.1739), ('mail', 0.15131)]\n",
      "Topic 3: [('game', 0.37159), ('team', 0.32533), ('year', 0.28205), ('games', 0.25416), ('season', 0.18464)]\n",
      "Topic 4: [('drive', 0.52823), ('scsi', 0.20043), ('disk', 0.15518), ('hard', 0.15511), ('card', 0.14049)]\n",
      "Topic 5: [('windows', 0.40544), ('file', 0.25619), ('window', 0.1806), ('files', 0.16196), ('program', 0.14009)]\n",
      "Topic 6: [('government', 0.16085), ('chip', 0.16071), ('mail', 0.15626), ('space', 0.15047), ('information', 0.13582)]\n",
      "Topic 7: [('like', 0.67121), ('bike', 0.14274), ('know', 0.11189), ('chip', 0.11043), ('sounds', 0.10389)]\n",
      "Topic 8: [('card', 0.44948), ('sale', 0.21639), ('video', 0.21318), ('offer', 0.14896), ('monitor', 0.1487)]\n",
      "Topic 9: [('know', 0.44869), ('card', 0.35699), ('chip', 0.17169), ('video', 0.15289), ('government', 0.15069)]\n",
      "Topic 10: [('good', 0.41575), ('know', 0.23137), ('time', 0.18933), ('bike', 0.11317), ('jesus', 0.09421)]\n",
      "Topic 11: [('think', 0.7832), ('chip', 0.10776), ('good', 0.10613), ('thanks', 0.08985), ('clipper', 0.07882)]\n",
      "Topic 12: [('thanks', 0.37279), ('right', 0.21787), ('problem', 0.2172), ('good', 0.21405), ('bike', 0.2116)]\n",
      "Topic 13: [('good', 0.36691), ('people', 0.33814), ('windows', 0.28286), ('know', 0.25238), ('file', 0.18193)]\n",
      "Topic 14: [('space', 0.39894), ('think', 0.23279), ('know', 0.17956), ('nasa', 0.15218), ('problem', 0.12924)]\n",
      "Topic 15: [('space', 0.3092), ('good', 0.30207), ('card', 0.21615), ('people', 0.20208), ('time', 0.15716)]\n",
      "Topic 16: [('people', 0.46951), ('problem', 0.20879), ('window', 0.16), ('time', 0.13873), ('game', 0.13616)]\n",
      "Topic 17: [('time', 0.3419), ('bike', 0.26896), ('right', 0.26208), ('windows', 0.19632), ('file', 0.19145)]\n",
      "Topic 18: [('time', 0.60079), ('problem', 0.15209), ('file', 0.13856), ('think', 0.13025), ('israel', 0.10728)]\n",
      "Topic 19: [('file', 0.4489), ('need', 0.25951), ('card', 0.1876), ('files', 0.17632), ('problem', 0.1491)]\n",
      "Topic 20: [('problem', 0.32797), ('file', 0.26268), ('thanks', 0.23414), ('used', 0.19339), ('space', 0.13861)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() \n",
    "# 단어 집합. 1,000개의 단어가 저장됨\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n -1:-1]])\n",
    "get_topics(svd_model.components_, terms)\n",
    "\n",
    "#각 20개의 행의 각 1,000개의 열 중 가장 값이 큰 5개의 값을 찾아서 단어로 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
